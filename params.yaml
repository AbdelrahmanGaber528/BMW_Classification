LogisticRegression:
  penalty: l2
  C: 1.0
  solver: lbfgs
  max_iter: 100

DecisionTreeClassifier:
  criterion: [gini, entropy, log_loss]
  splitter: [best, random]
  max_depth: [null, 5, 10, 15, 20, 30, 50]
  min_samples_split: [2, 5, 10, 20]
  min_samples_leaf: [1, 2, 4, 8]
  max_features: [sqrt, log2, null]

SVC:
  C: [0.01, 0.1, 1, 10, 100]
  kernel: [linear, poly, rbf, sigmoid]
  gamma: [scale, auto]
  degree: [2, 3, 4, 5]

RandomForestClassifier:
  n_estimators: [100, 200, 300, 500]
  criterion: [gini, entropy, log_loss]
  max_depth: [null, 10, 20, 30, 50]
  min_samples_split: [2, 5, 10]
  min_samples_leaf: [1, 2, 4]
  max_features: [sqrt, log2, null]

AdaBoostClassifier:
  n_estimators: [50, 100, 200, 300]
  learning_rate: [0.001, 0.01, 0.1, 1]
  algorithm: [SAMME, SAMME.R]

KNeighborsClassifier:
  n_neighbors: [3, 5, 7, 9, 11]
  weights: [uniform, distance]
  algorithm: [auto, ball_tree, kd_tree, brute]
  p: [1, 2]

XGBClassifier:
  n_estimators: [100, 200, 300]
  learning_rate: [0.01, 0.05, 0.1, 0.3]
  max_depth: [3, 5, 7, 10]
  min_child_weight: [1, 3, 5]
  gamma: [0, 0.1, 0.2, 0.5]
  subsample: [0.6, 0.8, 1.0]
  colsample_bytree: [0.6, 0.8, 1.0]
  reg_alpha: [0, 0.1, 1]
  reg_lambda: [1, 1.5, 2]

CatBoostClassifier:
  iterations: [100, 200, 300]
  learning_rate: [0.01, 0.05, 0.1, 0.3]
  depth: [4, 6, 8, 10]
  l2_leaf_reg: [1, 3, 5, 7]
  border_count: [32, 64, 128]
  verbose: [0]
  random_seed: [42]